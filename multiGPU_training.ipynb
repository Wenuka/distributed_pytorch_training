{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e2ae86-5c49-437b-be65-2bd499a44469",
   "metadata": {},
   "source": [
    "# Multi-GPU Training Demo (PyTorch)\n",
    "\n",
    "This notebook demonstrates how to accelerate model training using multiple GPUs in PyTorch.  \n",
    "We benchmark three execution modes:\n",
    "\n",
    "1. **Single GPU**\n",
    "2. **DataParallel (DP)** : multi-GPU within one process\n",
    "3. **DistributedDataParallel (DDP)** : multi-GPU using multiple processes\n",
    "\n",
    "We use a lightweight vision task:\n",
    "- **Model:** ResNet-18 (or ResNet-34)\n",
    "- **Dataset:** CIFAR-10\n",
    "\n",
    "For each mode, we measure:\n",
    "- **Time per epoch**\n",
    "- **Speedup relative to single-GPU training**\n",
    "- **Efficiency across two GPUs**\n",
    "\n",
    "**Disclaimer:** Parts of this notebook were assisted by GPT-5.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1edc72-14b9-41d5-b1ba-1cd0788334f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train_utils import get_model, get_cifar10_dataloaders, train_one_epoch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # speed up for fixed-size inputs\n",
    "\n",
    "MODEL_NAME = \"resnet18\" # \"resnet18\" , \"resnet34\"\n",
    "\n",
    "BATCH_SIZE_SINGLE = 512     # batch size for single GPU\n",
    "BATCH_SIZE_PER_GPU = 512    # per-GPU batch size\n",
    "EPOCHS = 2\n",
    "NUM_WORKERS = 4\n",
    "MAX_STEPS_PER_EPOCH = None  # set to e.g. 100 for quicker benchmarking\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"ALL\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b746c0-2659-4363-8c25-7f6eba75dc72",
   "metadata": {},
   "source": [
    "## Single GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7044c22-9cef-4b95-8272-78780e849bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_gpu(\n",
    "    model_name: str,\n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    max_steps_per_epoch: int | None = None,\n",
    "):\n",
    "    model = get_model(model_name).to(device)\n",
    "    train_loader, _ = get_cifar10_dataloaders(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        distributed=False,\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        avg_loss = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            max_steps=max_steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        epoch_time = end - start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"[Single] Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"loss: {avg_loss:.4f} - time: {epoch_time:.3f} s\")\n",
    "\n",
    "    return epoch_times\n",
    "if torch.cuda.is_available():\n",
    "    single_times = run_single_gpu(\n",
    "        MODEL_NAME,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE_SINGLE,\n",
    "        max_steps_per_epoch=MAX_STEPS_PER_EPOCH,\n",
    "    )\n",
    "    print(\"Single GPU epoch times:\", single_times)\n",
    "else:\n",
    "    single_times = None\n",
    "    print(\"Skipping run: No GPU available.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9523724-47e4-45da-ae26-0799affe69f9",
   "metadata": {},
   "source": [
    "## DataParallel training (2 GPUs on one node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf2b0c-fadf-42b8-b4be-d9d6e5d9141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataparallel(\n",
    "    model_name: str,\n",
    "    epochs: int,\n",
    "    per_gpu_batch_size: int,\n",
    "    max_steps_per_epoch: int | None = None,\n",
    "):\n",
    "    if torch.cuda.device_count() < 2:\n",
    "        raise RuntimeError(\"DataParallel demo requires at least 2 GPUs.\")\n",
    "\n",
    "    global_batch_size = per_gpu_batch_size * torch.cuda.device_count()\n",
    "    print(f\"Using DataParallel on {torch.cuda.device_count()} GPUs.\")\n",
    "    print(f\"Per-GPU batch size: {per_gpu_batch_size}, \"\n",
    "          f\"global batch size: {global_batch_size}\")\n",
    "\n",
    "    base_model = get_model(model_name).to(device)\n",
    "    model = nn.DataParallel(base_model)  # wraps across all visible GPUs\n",
    "\n",
    "    train_loader, _ = get_cifar10_dataloaders(\n",
    "        batch_size=global_batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        distributed=False,\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        avg_loss = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            device,\n",
    "            max_steps=max_steps_per_epoch,\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        epoch_time = end - start\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        print(f\"[DP] Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"loss: {avg_loss:.4f} - time: {epoch_time:.3f} s\")\n",
    "\n",
    "    return epoch_times\n",
    "\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    dp_times = run_dataparallel(\n",
    "        MODEL_NAME,\n",
    "        epochs=EPOCHS,\n",
    "        per_gpu_batch_size=BATCH_SIZE_PER_GPU,\n",
    "        max_steps_per_epoch=MAX_STEPS_PER_EPOCH,\n",
    "    )\n",
    "    print(\"DataParallel epoch times:\", dp_times)\n",
    "else:\n",
    "    dp_times = None\n",
    "    print(\"Skipping DP run: <2 GPUs available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d8595-b682-4315-8493-c99563ee4b24",
   "metadata": {},
   "source": [
    "## DDP \n",
    "\n",
    "Running the `ddp_train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfeb34-0b0e-4fc8-aefb-5c663070de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() >= 2:\n",
    "    ddp_output_file = \"ddp_results.json\"\n",
    "\n",
    "    ddp_cmd = (\n",
    "        f\"torchrun --nproc_per_node=2 ddp_train.py \"\n",
    "        f\"--model-name {MODEL_NAME} \"\n",
    "        f\"--epochs {EPOCHS} \"\n",
    "        f\"--per-gpu-batch-size {BATCH_SIZE_PER_GPU} \"\n",
    "        f\"--num-workers {NUM_WORKERS} \"\n",
    "        f\"--max-steps-per-epoch {MAX_STEPS_PER_EPOCH or -1} \"\n",
    "        f\"--output-file {ddp_output_file}\"\n",
    "    )\n",
    "\n",
    "    print(\"Running:\", ddp_cmd)\n",
    "    ret = os.system(ddp_cmd)\n",
    "    if ret != 0:\n",
    "        print(\"DDP run failed, return code:\", ret)\n",
    "        ddp_times = None\n",
    "    else:\n",
    "        with open(ddp_output_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        ddp_times = data[\"epoch_times\"]\n",
    "        print(\"DDP epoch times:\", ddp_times)\n",
    "else:\n",
    "    ddp_times = None\n",
    "    print(\"Skipping DDP run: <2 GPUs available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6bbfd-371f-4a14-accb-3da40f972d1b",
   "metadata": {},
   "source": [
    "## Compaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06f53e-ee08-4419-82fc-123a4545673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"single\": single_times,\n",
    "    \"dp\": dp_times,\n",
    "    \"ddp\": ddp_times,\n",
    "}\n",
    "\n",
    "print(\"Results (epoch times in seconds):\")\n",
    "for k, v in results.items():\n",
    "    print(k, \":\", v)\n",
    "\n",
    "# Simple speedup vs single GPU (using mean epoch time)\n",
    "\n",
    "def mean_or_none(x):\n",
    "    return float(np.mean(x)) if x is not None else None\n",
    "\n",
    "t_single = mean_or_none(results[\"single\"])\n",
    "t_dp     = mean_or_none(results[\"dp\"])\n",
    "t_ddp    = mean_or_none(results[\"ddp\"])\n",
    "\n",
    "print(\"\\nAverage epoch times:\")\n",
    "print(f\"Single: {t_single:.3f} s\" if t_single else \"Single: N/A\")\n",
    "print(f\"DP:     {t_dp:.3f} s\"     if t_dp     else \"DP: N/A\")\n",
    "print(f\"DDP:    {t_ddp:.3f} s\"    if t_ddp    else \"DDP: N/A\")\n",
    "\n",
    "if t_single and t_dp:\n",
    "    print(f\"\\nSpeedup DP vs Single:  {t_single / t_dp:.2f}x\")\n",
    "if t_single and t_ddp:\n",
    "    print(f\"Speedup DDP vs Single: {t_single / t_ddp:.2f}x\")\n",
    "\n",
    "# Optional: bar plot\n",
    "labels = []\n",
    "times = []\n",
    "if t_single:\n",
    "    labels.append(\"Single\")\n",
    "    times.append(t_single)\n",
    "if t_dp:\n",
    "    labels.append(\"DP (2 GPUs)\")\n",
    "    times.append(t_dp)\n",
    "if t_ddp:\n",
    "    labels.append(\"DDP (2 GPUs)\")\n",
    "    times.append(t_ddp)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, times)\n",
    "plt.ylabel(\"Avg epoch time (s)\")\n",
    "plt.title(f\"ResNet ({MODEL_NAME}) on CIFAR-10: Single vs DP vs DDP\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a266be1-ca73-40cd-acf6-8dc0f2a1ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
